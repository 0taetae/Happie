## 2025.03.04 : 아이디어 구상
- 스마트홈
- 스마트물류/팩토리
- 스마트병원

## 2025.03.05 : 아이디어 구체화
- 스마트팜
    - 공간 관리
        - 물 주기, 텃밭 밀기 등등
    - 사용자와 연결
        - 사진 전송하여 텃밭의 상태 전달
- 스마트병원
    - 링거 홀대 + 휠체어 + 보행보조기 등 방치되는 기자재 관련 정리 및 위치 파악
    - 낙상 체크 및 알림
    - 간호사 호출 버튼
- 실버타운
    - 환경 관리
        - 타운 내 공원/정원/잔디 관리
        - 파크볼, 골프, 테니스 공 등 기자재 관리
    - 거주지 관리
        - 링거 홀대 / 휠체어 / 보행보조기 등 의료보조기 관리
        - 치매 예방이나 민원 기입 등

## 2025.03.06 : 팀 그라운드룰 설정, 사전학습
- 시뮬레이터
- ROS2
- Differential Drive
- OpenCV
- IMAGE 형식
- Custom Object
- Hand Control
- IMU
- 주행기록계(Odometry)
- Follow the carrot

## 2025.03.07 : ros2, 시뮬레이터 적응하기
- setup.bat, local_setup.bat을 call 하기
- colcon build 하기
- 시뮬레이터를 연결하여 데이터 확인하기

## 2025.03.10 : 요구사항 정의서, 시나리오 작성 

## 2025.03.11 : 와이어프레임, UI 제작 중

## 2025.03.12 : UI 1차 제작 완료, 역할 분배
![image](/uploads/2db1f2eaf1ea85a7a719dc3445d04ed2/image.png){width=906 height=311}

## 2025.03.13 : 시뮬레이터 1차 제작 확인, 자율주행 사전학습

## 2025.03.14 : UI 제작 수정, 시나리오 구체화 및 수정 

## 2025.03.17 : SLAM 학습
### SLAM의 개념
- 로봇이나 자율주행 차량이 주변 환경을 실시간으로 인식하고 자신의 위치를 추정하며 지도를 작성하는 기술
1. Localization (위치 추정)
	- 로봇이 현재 어디에 있는지를 추정하는 과정
	- GPS, IMU, LiDAR, 카메라 센서 등을 활용
2. Mapping (지도 작성)
	- 로봇 주변의 환경을 매핑하여 지도를 만드는 과정
	- 2D/3D 공간 정보를 수집하여 맵을 구축

### SLAM의 특징
- 실시간으로 Localization과 Mapping이 가능해야 한다.
- SLAM이 시작됨과 동시에 자신의 위치를 추정할 수 있다.
- SLAM이 종료됨과 동시에 자신의 이동기록계와 Map을 얻을 수 있다.
- 나의 위치가 전에 와본 위치인 것을 인식하고 Map의 오차 수정을 할 수 있다. 

### SLAM의 작동 원리
1. 예측 단계
	- 이전 위치와 센서가 제공하는 움직임 정보를 기반으로 로봇의 새 위치를 추정
	
2. 보정 단계
    - 환경에서의 측정값을 사용하여 예측된 위치를 정제

-> 두 단계를 반복적으로 수행하여 로봇의 위치와 환경의 지도를 지속적으로 업데이트한다.

### SLAM의 유형
1. Visual SLAM
	- 카메라 및 기타 영상 센서를 사용하여 주변 환경의 특징을 인식한다. 
    - 매 프레임마다 보이는 장애물과 사용중인 카메라의 상대적 위치를 계산해 누적함으로써, 시작점으로부터 현재의 나의 위치를 저장하는 방식 
    - 카메라 또는 IMU가 내장되어 있는 카메라 하나로 SLAM을 진행할 수 있다.
    - 카메라로는 주변 특징점 거리 파악을 하는 기능이 없거나 제한적이다. 
2. LiDAR SLAM
	- 3D 포인트 클라우드를 생성하여 주변 환경을 매핑한다.
    - 각 장면에 대한 특징점을 포인트 클라우드 형식으로 계산한다. 
3. 다중 센서 SLAM 
	- 여러 종류의 센서를 결합하여 정확성을 높인다.

## 2025.03.18 : 로봇 상태메시지, LiDAR 센서 값, 카메라 확인, Hand Control 테스트 

## 2025.03.20 : sub1 완료

## 2025.03.21 : 중간발표, sub2 학습
### Dijkstra 알고리즘
- 출발 노드에서 **모든 노드까지**의 최단 거리를 구하는 알고리즘
### A* 알고리즘 
- 시작 노드에서 **목적지 노드까지**의 최단 경로를 구하는 알고리즘 
- 현재 상태의 비용을 g(x), 현재 상태에서 다음 상태로 이동할 때의 **휴리스틱 함수**를 h(x)라고 할 때, f(x) = g(x) + h(x)가 최소가 되는 지점을 우선적으로 탐색하는 방법
### Extrinsic calibration
LiDAR와 카메라의 위치를 기준으로 물체를 측정/인식 측정하기 때문에, 자율주행차는 각 센서가 마운트된 **위치**와 **각도**를 명시해주고 그 마운트 정보를 가지고 각각의 센서에서 나오는 측정값들의 좌표를 하나로 통합시켜야 한다.
### 좌표 변환
1. 변환행렬의 평행이동과 회전
   - 평행이동
      - 이전 좌표계와 이동 후 좌표계 간의 x, y, z 방향 벡터를 가짐 
   - 회전
      - X, Y, Z 축을 기준으로 돌리는 오일러각으로 표현(roll, pitch, yaw)
2. LiDAR-카메라간 좌표 변환 순서
   a. LiDAR 좌표에서 카메라 좌표로 Transition matrix를 적용
   b. x축이 오른쪽으로 향하도록 z축 기준 90도 시계방향으로 회전
   c. z축이 카메라 정면을 향하도록 x축 방향으로 180도만큼 시계방향으로 회전
   -> 최종적으로 계산된 행렬은 카메라 좌표의 x,y,z 벡터를 LiDAR 좌표계로 변환시켜주는 역할
      역행렬은 LiDAR 스캔 포인트 좌표들을 카메라 좌표계 기준으로 변환시킴 
3. Projection
   - 카메라에 들어오는 2차원 정보와 LiDAR의 포인트 클라우드의 3차원 정보를 병합시키려면, 3차원 정보를 2차원 평면 상으로 투영(Projection)시켜야 함
   - 변환행렬들을 사용하여 전체 좌표계에서 카메라 좌표계로 좌표변환을 함
   - Normalized plane 상에서 투영되었으면, 현재 사용하고 있는 카메라의 파라미터를 가지고 Intrinsic matrix를 만들어야 함.
   



