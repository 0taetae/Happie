# README

## 2025.03.07 학습 내용

### 1. IMU (Inertial Measurement Unit)
IMU는 관성 센서로, 회전하거나 일정한 비율로 가속되는 기준좌표계 안에서 뉴턴의 제2 법칙을 만족하도록 가상적으로 도입한 힘을 측정합니다. IMU는 가속도 센서와 자이로 센서로 구성됩니다. 이들 센서는 서로의 부족한 점을 보완하며, 로봇의 자세를 추정하는 데 사용됩니다. 자세는 이동(Translation)과 회전(Rotation)으로 정의되며, 로봇은 3 DOF(자유도)를 가집니다. 비행기의 경우 6 DOF입니다.

- **자이로 센서**: 회전하는 물체의 각속도를 측정하며, 각속도를 적분하여 각도를 계산할 수 있습니다.
- **가속도 센서**: 물체의 가속도와 진동을 측정합니다. 가속도 센서만으로는 기울기를 정확히 알 수 없기 때문에 자이로 센서가 보완합니다.

IMU의 출력은 Quaternion을 사용하여 오일러 각(roll, pitch, yaw)으로 변환할 수 있습니다. Quaternion은 오일러 각에서 발생할 수 있는 짐벌락 현상을 방지하는 장점이 있어, 주로 컴퓨터 그래픽스에서 사용됩니다.

#### 관련 링크:
- [오일러 각](https://ko.wikipedia.org/wiki/%EC%98%A4%EC%9D%BC%EB%9F%AC%20%EA%B0%81)
- [squaternion 파이썬 모듈](https://pypi.org/project/squaternion/)
- [IMU ROS 메시지](https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/Imu.html)
- [IMU Wiki](https://en.wikipedia.org/wiki/Inertial_measurement_unit)

### 2. 주행기록계 (Odometry)
주행기록계는 로봇의 기구학 모델을 이용하여 로봇의 이동을 추정합니다. 이때 추정되는 위치는 절대적인 위치가 아니라, 주행기록을 시작한 위치를 기준으로 한 상대 위치입니다. 로봇의 각 바퀴 속도와 선속도, 각속도를 이용하여 자세(x, y, δ)의 변화량을 구하고 이를 누적시켜 Odometry를 생성합니다.

#### Kinematic Odometry
로봇에 부착된 모터의 속도를 측정하여 주행기록을 추정합니다. 주행에 따라 오차가 누적될 수 있으며, 이를 보정하기 위해 IMU에서 측정한 각속도 또는 Quaternion을 활용할 수 있습니다.

#### 관련 링크:
- [Kinematics](https://en.wikipedia.org/wiki/Kinematics)
- [Visual Odometry](https://en.wikipedia.org/wiki/Visual_odometry)
- [Odometry YouTube](https://www.youtube.com/results?search_query=odometry)

### 3. 경로 (Path Planning)
경로는 로봇이 주행한 기록을 저장하고, 이를 기반으로 테스트할 수 있는 방법입니다. 경로에는 전역 경로와 지역 경로가 있으며, 전역 경로는 출발점에서 목적지까지의 전체적인 경로를 의미하고, 지역 경로는 경로의 일부를 수정하여 새로운 경로를 만드는 방식입니다.

#### Follow the Carrot
이동 로봇에 사용되는 기하학적 경로 추종 방법으로, 로봇이 목표거리만큼 떨어진 목표지향점을 향해 경로를 추종합니다. 경로 추종을 위한 각도와 속도는 목표지향점과 로봇의 위치 및 방향을 사용해 계산됩니다.

#### 관련 링크:
- [전역경로, 지역경로 계획](https://ko.wikipedia.org/wiki/%EA%B2%BD%EB%A1%9C_%ED%8F%AC%EB%8F%84_%EA%B5%AC%ED%98%84)

### 4. 기타 센서 기반 주행기록 (Visual Odometry, Lidar Odometry)

#### Visual Odometry
카메라를 이용하여 환경에서 추출된 특징점(feature)을 기반으로 이동 경로를 추정합니다. 특징점이 뚜렷한 환경에서 효과적입니다.

#### Lidar Odometry
Lidar 센서를 이용하여 환경의 변화를 기반으로 주행기록을 추정합니다. Lidar는 포인트들에 다양한 특징을 갖고 있어, 복잡한 환경에서 정확한 주행기록 추정이 가능합니다.

#### 오차 보정
Kinematic Odometry의 경우, 지면과의 마찰력 문제나 모터의 공회전으로 인해 오차가 누적될 수 있으며, 이를 IMU 데이터를 활용해 보정할 수 있습니다.

---

## 2025.03.10 - 2025.03.11 학습 내용

### 1. 자율주행
- **Map 감지 및 생성**: SLAM 알고리즘 활용
- **Map 구현도 추출**
- **자유 이동 기능** (Optional)
- **장애물 인식 및 회피**

### 2. 낙상감지
- **낙상 상태 감지**
- **알림 전송**
- **의식 확인 도움 기능 (추가 가능)**

### 3. 기자재 판별
- **카메라 인식을 통한 기자재 식별**
- **적재 가능 여부 판별**

### 4. 운송
- **출발지 ~ 도착지 경로 이동**: 최단거리 탐색 및 경로 최적화

### 5. 의약품 분류
- **바코드 기반 카메라 인식**
- **의약품 자동 분류 및 적재**

### 6. 음성 명령 처리
- **STT (Speech-to-Text) → 자연어처리(NLP) → TTS (Text-to-Speech)**
- **예제 명령어**:
  - “어디로 가~” → 목적지 설정
  - “의료진 호출해줘~” → 의료진 호출 기능 실행

#### 관련 링크:
- [요구사항 정의서](https://www.notion.so/1a4bcdc0fb5380e1a72fd81503ec59e1?pvs=21)
- [유스케이스 시나리오](https://www.notion.so/1b2bcdc0fb5380498c4ccf106aca73a3?pvs=21)


## 2025.03.12 학습 내용

### 1. SLAM (Simultaneous Localization and Mapping)
SLAM은 로봇이 주변 환경을 탐색하면서 지도(Map)를 생성하고, 동시에 자신의 위치(Localization)를 추정하는 기술입니다. 이를 구현하는 방법으로 Lidar 기반 SLAM과 **Visual SLAM(V-SLAM)**이 있습니다.

#### (1) Lidar 기반 SLAM
Lidar 센서를 활용하여 주변 환경을 스캔하고, 이를 기반으로 실시간으로 지도를 생성하는 방식입니다.

**장점**
- 정밀한 거리 측정이 가능하여 고해상도 지도 생성이 용이함
- 조명 변화나 환경의 복잡성에 영향을 덜 받음
- 실내 및 실외 환경에서 안정적으로 동작 가능

**단점**
- Lidar 센서는 가격이 비쌀 수 있음
- 점 데이터(Point Cloud) 기반으로 환경을 인식하기 때문에 세부적인 객체 정보는 부족함
- 반사율이 낮은 표면(유리, 물 등)에서는 정확도가 떨어질 수 있음

#### (2) Visual SLAM (V-SLAM)
카메라를 활용하여 영상을 기반으로 SLAM을 수행하는 방식입니다. 특징점(Feature)을 추출하고, 이를 기반으로 로봇의 위치를 파악합니다.

**장점**
- 저렴한 카메라만으로도 구현 가능하여 비용 효율적
- 환경의 시각적 정보를 활용할 수 있어 물체 인식이 가능
- 사람이 보는 방식과 유사하여 의미 있는 환경 정보를 얻을 수 있음

**단점**
- 조명 변화(어두운 곳, 역광)와 동적 환경에서 성능 저하 가능
- 고속 이동 시 모션 블러(Motion Blur)로 인해 정확도가 떨어질 수 있음
- 계산량이 많아 실시간 처리가 어려울 수도 있음

#### (3) Lidar SLAM vs V-SLAM 비교 정리

| 구분 | Lidar SLAM | V-SLAM |
|------|-----------|--------|
| **센서 종류** | Lidar 센서 | 카메라 |
| **거리 측정 정확도** | 높음 | 상대적으로 낮음 |
| **환경 영향** | 조명 영향 없음 | 조명 변화에 취약 |
| **비용** | 상대적으로 고가 | 저렴한 장비 사용 가능 |
| **실내/실외 활용** | 실내외 모두 활용 가능 | 조명 영향을 받지만 다양한 환경에서 활용 가능 |
| **객체 인식** | 불가능 (3D Point Cloud 기반) | 가능 (이미지 기반) |

### 2. 장애물 회피 (Obstacle Avoidance)
SLAM과 함께 중요한 요소는 장애물 감지 및 회피 기능입니다. 로봇이 이동할 때 장애물을 감지하고 경로를 재설정하는 방식이 있습니다.

#### 장애물 감지 센서 종류
- **Lidar 센서**: 거리 기반 장애물 감지
- **RGB-D 카메라**: 컬러 및 깊이 정보를 활용한 감지
- **초음파 센서**: 가까운 거리에서 장애물 탐지

#### 장애물 회피 알고리즘
- **Potential Field Method**: 로봇이 목표 지점으로 이동하는 동안 장애물의 영향을 고려하여 이동
- **Dijkstra & A* 알고리즘**: 장애물이 있는 환경에서 최단경로를 탐색하여 이동
- **Dynamic Window Approach (DWA)**: 로봇의 동적 속도와 장애물을 고려하여 실시간 경로 수정

### 3. 로봇 내비게이션 (Robot Navigation)
로봇이 목적지까지 안전하고 최적의 경로로 이동하기 위해 내비게이션 기술이 필요합니다.

#### 주요 구성 요소

##### 1) 경로 계획 (Path Planning)
- **전역 경로(Global Path Planning)**: 전체적인 이동 경로를 생성 (A*, Dijkstra 알고리즘 활용)
- **지역 경로(Local Path Planning)**: 실시간으로 장애물을 고려하여 회피

##### 2) 위치 추정 (Localization)
- **Monte Carlo Localization (MCL)**: 확률 기반으로 위치 추정
- **Kalman Filter**: 센서 데이터를 융합하여 위치를 보정

##### 3) 환경 인식 (Perception)
- 장애물 감지 및 주변 환경 매핑

#### 예제 사용 시나리오
- 창고 내에서 물품을 자동으로 이동하는 물류 로봇
- 병원에서 의료 물품을 운반하는 자율주행 카트
- 공장에서 자율 이동 로봇(AMR, Autonomous Mobile Robot) 적용

### 4. 추가 학습 내용

#### VIO (Visual-Inertial Odometry)
- V-SLAM과 IMU 센서를 결합하여 위치 추정을 보정하는 방식

#### Multi-SLAM
- 여러 대의 로봇이 협력하여 SLAM을 수행하고 지도를 공유하는 방식

#### Semantic SLAM
- SLAM과 AI를 결합하여 환경을 단순 지도 생성이 아니라 의미를 해석하는 방식 (예: ‘이것은 책상’, ‘이것은 문’)

#### 음성 인식 내비게이션
- STT(Speech-to-Text)를 활용한 음성 명령 기반 이동 (예: “회의실로 가”, “응급실로 가”)

